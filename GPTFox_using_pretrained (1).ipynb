{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPTFox_using_pretrained",
      "provenance": [],
      "collapsed_sections": [
        "bxJg-pHQzzil"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_UWz_d2Ou9"
      },
      "source": [
        "# 🤖 Load a pretrained GPT and generate text! 🤖\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRRkFgGej8Up",
        "outputId": "0030a47c-0cee-414c-a871-016756374cf7"
      },
      "source": [
        "#@title #**DASHBOARD**\r\n",
        "#@markdown ---\r\n",
        "FOLDER_NAME = \"GPTFox\" #@param {type:\"string\"} \r\n",
        "MODEL_NAME = \"GeppettoFox_1\" #@param {type:\"string\"}\r\n",
        "PRETRAINED = \"LorenzoDeMattei/GePpeTto\" #@param [\"LorenzoDeMattei/GePpeTto\"]\r\n",
        "\r\n",
        "####################################\r\n",
        "DRIVE_FOLDER = f\"/content/gdrive/My Drive/{FOLDER_NAME}/\"\r\n",
        "DRIVE_OUTPUT_NAME = f\"{DRIVE_FOLDER}outputs_{MODEL_NAME}.txt\"\r\n",
        "DRIVE_ARCHIVE_PATH = DRIVE_FOLDER + MODEL_NAME + \".zip\"\r\n",
        "LOCAL_MODEL_PATH = f\"./{MODEL_NAME}\"\r\n",
        "\r\n",
        "#@markdown ---\r\n",
        "#@markdown ##Text generation settings\r\n",
        "PROMPTS = \"Bilancia; Scorpione; Saggittario\" #@param {type:\"string\"}\r\n",
        "PROMPTS = PROMPTS.split(\";\")\r\n",
        "SAMPLES_PER_PROMPT = 1 #@param {type:\"number\"}\r\n",
        "MAX_LENGTH = \"280 (Twitter max)\" #@param [\"280 (Twitter max)\", 100]\r\n",
        "MAX_LENGHT = int(MAX_LENGTH.split(\" \")[0])\r\n",
        "\r\n",
        "#@markdown ---\r\n",
        "#@markdown ##Export settings\r\n",
        "OUTPUT_FILE_NAME = f\"{LOCAL_MODEL_PATH}/outputs_{MODEL_NAME}.txt\"\r\n",
        "SAVE_ON_DRIVE = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "\r\n",
        "####################################\r\n",
        "print(f\" SUMMARY:\\n{'-'*100}\")\r\n",
        "print(f\" Drive model archive path:        {DRIVE_ARCHIVE_PATH}\")\r\n",
        "print(f\" Local model path:                {LOCAL_MODEL_PATH}\")\r\n",
        "print(f\" Outputs file name (local):       {OUTPUT_FILE_NAME}\")\r\n",
        "print(f\" Outputs file name (drive):       {DRIVE_OUTPUT_NAME.replace('./', '')}\")\r\n",
        "print(f\" sample length                    {MAX_LENGHT}\")\r\n",
        "print(f\" prompts: \")\r\n",
        "PROMPTS = [f\"{p.strip()}\" for p in PROMPTS]\r\n",
        "for p in PROMPTS:\r\n",
        "  print(f\"  > \", p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SUMMARY:\n",
            "----------------------------------------------------------------------------------------------------\n",
            " Drive model archive path:        /content/gdrive/My Drive/GPTFox/GeppettoFox_1.zip\n",
            " Local model path:                ./GeppettoFox_1\n",
            " Outputs file name (local):       ./GeppettoFox_1/outputs_GeppettoFox_1.txt\n",
            " Outputs file name (drive):       /content/gdrive/My Drive/GPTFox/outputs_GeppettoFox_1.txt\n",
            " sample length                    280\n",
            " prompts: \n",
            "  >  Bilancia\n",
            "  >  Scorpione\n",
            "  >  Saggittario\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxJg-pHQzzil"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI-qnAZuzx7U",
        "outputId": "09052192-dc4b-472f-b03b-1eff00bb690f"
      },
      "source": [
        "########################################################################\r\n",
        "############################ S E T U P #################################\r\n",
        "########################################################################\r\n",
        "from google.colab import drive\r\n",
        "import shutil\r\n",
        "\r\n",
        "if SAVE_ON_DRIVE:\r\n",
        "  drive.mount('/content/gdrive')\r\n",
        "  print(f\" unpacking archive {DRIVE_ARCHIVE_PATH}...\")\r\n",
        "  shutil.unpack_archive(DRIVE_ARCHIVE_PATH, LOCAL_MODEL_PATH)\r\n",
        "  print(f\" successfully unpacked in {LOCAL_MODEL_PATH}\")\r\n",
        "\r\n",
        "\r\n",
        "!pip install transformers\r\n",
        "!nvidia-smi\r\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\r\n",
        "\r\n",
        "# importing model\r\n",
        "model = AutoModelWithLMHead.from_pretrained(LOCAL_MODEL_PATH)\r\n",
        "print(f\"Model loaded: '{MODEL_NAME}'.\")\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\r\n",
        "print(\"Tokenizer loaded.\")\r\n",
        "\r\n",
        "# clone git repository\r\n",
        "import sys\r\n",
        "!git clone \"https://github.com/RiccardoCozzi96/italian_horoscope_generator\"\r\n",
        "sys.path.append(GITHUB_REPO_NAME+\"/\")\r\n",
        "\r\n",
        "# utils function definition\r\n",
        "def clean(text):\r\n",
        "  a = text[:text.find(\"]\")+1]\r\n",
        "  b = text[text.find(\"]\"):]\r\n",
        "  c = b[:b.find(\"[\")].replace(\"]\", \"\")\r\n",
        "  c = c.replace(\"\\n\", \" \").strip()\r\n",
        "  return (a[1:-1], c)\r\n",
        "\r\n",
        "def format(sign_body):\r\n",
        "  sign, body = sign_body\r\n",
        "  return f\"[{sign}]\\n{body}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            " unpacking archive /content/gdrive/My Drive/GPTFox/GeppettoFox_1.zip...\n",
            " successfully unpacked in ./GeppettoFox_1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Sun Jan 31 17:29:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded: 'GeppettoFox_1'.\n",
            "Tokenizer loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZ_sk5wnqCn"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3poRRZixnX3Q",
        "outputId": "6300c524-36f0-42fb-ef52-7d06426d9121"
      },
      "source": [
        "generator = pipeline('text-generation', model=LOCAL_MODEL_PATH, tokenizer=tokenizer, config={'max_length':800})\r\n",
        "print(\"Generator created.\")\r\n",
        "\r\n",
        "# generate text\r\n",
        "print(f\"Generation on prompts {PROMPTS}...\", end=\" \")\r\n",
        "samples_outputs = generator(\r\n",
        "    PROMPTS,\r\n",
        "    do_sample=True,\r\n",
        "    max_length=MAX_LENGHT,\r\n",
        "    top_k=50,\r\n",
        "    top_p=0.95,\r\n",
        "    num_return_sequences=SAMPLES_PER_PROMPT\r\n",
        ")\r\n",
        "print(\"completed\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator created.\n",
            "Generation on prompts ['[Bilancia]', '[Scorpione]', '[Saggittario]']... completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4KjiIGJrPJB"
      },
      "source": [
        "###Organize outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EseqUeqWq9CC",
        "outputId": "c1aa2e27-eec1-4ae5-8209-e890950cf20e"
      },
      "source": [
        "# save cleaned texts\r\n",
        "outputs = []\r\n",
        "for prompt, sample in zip(prompts, samples_outputs):\r\n",
        "  text = clean(sample[0][\"generated_text\"].replace(\"\\n\", \" \"))\r\n",
        "  line = f\"{prompt} {text[1]}\"\r\n",
        "  outputs.append(line)\r\n",
        "\r\n",
        "for output in outputs:\r\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Bilancia] non esiste, questo è la verità, ma l'ultimo anello che ho trovato in questa discussione era che l'oggetto dell'analisi del teorema di Pitagora era un altro.\n",
            "[Scorpione] Azzurra, 18 anni, Torino (TO). (9 Febbraio 2005) Se fossi stato un'attrice avrei dovuto scrivere un fumetto, ma quando ho visto la sua versione mi sono trovato comunque molto bene. E' stato un grande piacere vederlo. A parte certi piccoli difetti che in questo forum ci sono e ci sono, sono un ottimo punto di partenza per iniziare a realizzare le altre storie. La grafica è molto piacevole e quindi ci sono parecchi punti che non fanno paura, è difficile trovare personaggi che mi fanno sorridere, ma per di sicuro non riuscire a vedere le situazioni ci vuole.  La prima storia del personaggio di Patammina è semplice è piuttosto un personaggio più divertente. Anche nelle sue caratteristiche che mi ha molto di una persona un personaggio, e lo è di grande. Questo è molto simile agli altri personaggi che è il fatto a livello.      Da notare di personaggio di non so aver incontrato, ma la sua moglie che si ama il personaggio, ma per fare di quello che sono una persona.     per la mia amica e di una persona che è una persona che si nota, il desiderio è una persona da ricordare ancora, non essere come un altro.        per non avere sempre di una persona più dolce, quindi a volte di avere una persona che non avere una persona che\n",
            "[Saggittario] Il mio lavoro, come tutti, è quello di mettere le mani avanti, di far emergere un problema. Per me sarebbe tutto molto importante perché per me questo è l'unica realtà, come tutti. Non è possibile essere \"perfettamente liberi\" da tutto ciٍ. Se per qualche ragione non si possono fare tante cose, allora si sta davvero a fare qualcosa che le persone più comuni non possono capire\".  Links http://www.stranieriinitalia.it/news/mar2003/mar2003-10mar03.htm  Da tempo imho, il mio carattere e lo dico di una persona, di quelle persone che hanno fatto i numeri di cartacce... forse (ho letto, che prima di persona, come una specie di donne e di cui sono state tutte quelle che si sta parlando) non hanno preso tante volte è stata a cuore da tempo... io o meno, ma che si sono diventate. che le persone, per fare ancora, di discorsi mai, che sono state veramente: non riesco a letto che a fare le parole come a sentire, di queste persone che adesso, che sono state; ed è successo a parlare, quindi a sentire solo al giorno, con la stessa, ma come me, che sono ancora. non hanno dato sempre, a cui non si sono per essere sempre, sempre ma non è stato. Mi capita sol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrMuPFPS0G19"
      },
      "source": [
        "### Export results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwm7w-LJt83O",
        "outputId": "640abbde-9aaa-41b2-f6bb-94ef38a4fa20"
      },
      "source": [
        "# save results in local\r\n",
        "with open(OUTPUT_FILE_NAME, \"w\") as f:\r\n",
        "  for output in outputs:\r\n",
        "    f.write(output + \"\\n\")\r\n",
        "print(f\"Results saved locally at \\t{OUTPUT_FILE_NAME}\")\r\n",
        "\r\n",
        "# save results on drive\r\n",
        "if SAVE_ON_DRIVE:\r\n",
        "  shutil.copy(OUTPUT_FILE_NAME, DRIVE_OUTPUT_NAME)\r\n",
        "  print(f\"Results saved on drive at \\t{DRIVE_OUTPUT_NAME}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results saved locally at \t./GeppettoFox_1/outputs_GeppettoFox_1.txt\n",
            "Results saved on drive at \t/content/gdrive/My Drive/GPTFox/outputs_GeppettoFox_1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m434oLo1_37"
      },
      "source": [
        "## Done! Enjoy your generated texts 🤖"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t1rveVDzIAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0055aa7-0c32-4fb9-9cdf-ac2e54f15d93"
      },
      "source": [
        "print(\"you can find your results at these locations:\")\r\n",
        "print(\" [local]\\t\"+OUTPUT_FILE_NAME)\r\n",
        "if SAVE_ON_DRIVE: \r\n",
        "  print(\" [drive]\\t\"+DRIVE_OUTPUT_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you can find your results at these locations:\n",
            " [local]\t./GeppettoFox_1/outputs_GeppettoFox_1.txt\n",
            " [drive]\t/content/gdrive/My Drive/GPTFox/outputs_GeppettoFox_1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWhnPpuLB9OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad4e0a8-8bce-497a-f461-86637b76eb18"
      },
      "source": [
        "with open(OUTPUT_FILE_NAME) as results:\r\n",
        "  print(results.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Bilancia] Ciٍ significa che il risultato e' solo quello, in cui le masse in gioco hanno la meglio. Questo succede in base al ragionamento. Si potrebbe pensare di utilizzare la calcolatrice come strumento per compiere azioni o agire con maggior attenzione ma non si puo' prevedere a priori che la strategia o il risultato di tale azione dipende dalle masse stesse. E' un'analisi, questa, che ci permette di porre un problema. Ci rendiamo conto del fatto che noi dobbiamo essere realistici a partire da questa considerazione. Infatti è necessario che ciascuno dia un contributo fondamentale alla propria idea e di cui vuole farcela. E' fare attenzione quando gli altri. E' cosa vuole farlo, ma non lo fa molto meglio. Che non sta nel momento in maniera giusta. Quando l' opera bene, se stessa, non si sente solo. Anche la stessa. Ora si fa' il risultato. Quando sta male. Se si sente meglio quando non si sente meglio che si capisce di continuare a farlo bene. Riprendono, senza la frase che lo sta male. Quando lo fa bene. E' e si sta giocando con il comportamento. Infatti si crede che è un' a sufficienza deve cercare sempre nel giusto. Anche quando deve essere meglio, allora, altrimenti non si sta dicendo che gli si sbaglia. E' quando e che i tempi sbagliando l' quando deve prendere. E' i\n",
            "[Scorpione] A chi si rivolgeva questa lettera? A chi? A un cittadino italiano? A chi non si rivolgeva questa lettera? A chi? A nessuno? Alla famiglia? A chi non si rivolge questa lettera? A nessuno? Qual è il rapporto tra il cittadino e i suoi servizi? A nessuno, al loro interno, puٍ essere interessato un rapporto a parte. Vi è bisogno di un'offerta per poter avere la garanzia di un'assistenza di qualità per una persona di una certa importanza e per una persona che ha l'obiettivo di raggiungere un livello di benessere maggiore benessere. Chi puٍ avere una soluzione? Chi puٍ rifiutare di una soluzione migliore e per poter sostenere una decisione? E' ancora a lungo tempo nel momento? A chi ha voglia di prendere delle alternative? Chi risponde che si potrà essere in futuro? A quale condizione? Questo momento di essere disponibile per altre alternative? La domanda deve essere pronto e se una risposta La persona? Come fare l'avere sempre più vicina a chi non potrà rifiutare la risposta? L'altra scelta migliore. In questo. Non sono in un percorso di vita per avere una risposta a chi è una scelta per un podere ancora, una scelta. Che abbia una scelta? Quale soluzione Si potrebbe cambiare i propri posti in un'altra risposta Una questione. La soluzione a cui le stesse posizioni? Le persone. L'offert\n",
            "[Saggittario] \"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}